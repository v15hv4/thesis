\section{Dataset Curation for Training and Evaluation}
To construct training and evaluation sets, we curated peptide structures from the PepBDB database \cite{Wen2019}. PepBDB (Peptide Binding DataBase) is a curated structural database specializing in biological peptide-protein interactions \cite{Wen2019}. It provides clean data for structure-based peptide drug design, particularly for docking and scoring studies. Compiled from the Protein Data Bank (PDB), PepBDB focuses on structures of interacting peptide-protein complexes, with peptides limited to 50 amino acid residues in length. Regular monthly updates ensure the database reflects the latest data released in the PDB. \\

We curated 9225 protein-peptide complexes for the training dataset, 200 complexes for the validation dataset as well as 193 complexes for the test dataset. The complete dataset comprising 9618 protein-peptide complexes contained 1082 complexes that shared receptors with a subset of 504 unique receptors. To minimize overfitting and training set bias, the test set was carefully selected to have minimal overlap with the receptor proteins present in the train set. This resulted in the test set of 193 complexes with only 7 complexes sharing receptor proteins found in the training set. This minimal overlap between the training and test set data assures that our evaluation metric values are not a product of overfitting or train set bias. Table TODO summarizes the analysis of various physicochemical properties of peptides found in the dataset. \\

To ensure efficient computational processing and focus on the key interaction regions, a data preprocessing pipeline was implemented. The first step involved outlier removal based on protein size. Complexes containing exceptionally large proteins were excluded by imposing a maximum atom count threshold of 200 per protein. This filtering step mitigated potential memory limitations during subsequent deep learning model training. Next, to isolate the crucial binding pockets within the protein structures for analysis, the full protein structures were strategically sectioned. This was achieved by extracting a circumscribed spherical region with a radius of 10Ã… centered around the bound peptide. This approach streamlined the data and made the process more computationally efficient by focusing solely on the most relevant interaction interfaces critical for peptide binding. \\

The preprocessed dataset was then partitioned into distinct sets to prevent overfitting and enable robust evaluation of the trained HYDRA model. A hold-out test set comprising 2\% (193 complexes) of the entire dataset was established. This unseen test set served as a completely novel benchmark for unbiased model performance assessment. Furthermore, a validation set of 200 complexes (approximately 2\% of the remaining data) was designated for continuous monitoring of the training process and hyperparameter optimization. Finally, the remaining 9225 complexes constituted the primary training set. \\

To assess HYDRA's ability to generalize to unseen receptors, we focused on \textit{de novo} peptide binder generation (novel peptides not encountered during training) for each of the 193 binding pockets within the independent test set. For each pocket, we generated 30 unique peptides, resulting in a total of 5790 novel peptides to evaluate HYDRA's binding prediction for unseen receptors.

\section{Model Architecture and Specifications}
The target-aware in-place residue generation step is implemented through a SE(3)-equivariant Graph Neural Network to model the interaction between the peptide residuals and the target protein atoms. The key/value embedding and attention scores are generated through a 2-layer Multi Layer Perceptron with LayerNorm and ReLU activation.
The SE(3)-equivariant network contains 9 equivariant layers where $f_h$ and $f_x$ are implemented as graph attention layers for features and coordinates with 16 attention heads and 128 hidden features. We initiated our architecture and hyperparameter search using values reported in related literature \cite{guan20233d} as a starting point. Subsequently, we conducted a series of experiments with multiple configurations, systematically adjusting and refining these parameters. Through this iterative process of testing and optimization, we ultimately arrived at the current set of model parameters that yielded the best performance. Figures pertaining to the neural network architecture iteration are given below. (TODO: figures from wandb) \\

For the peptide reconstruction process, the Binary Particle Swarm Optimization (Binary PSO) algorithm is used. This variant of the standard PSO algorithm restricts particles to binary values (0 or 1). This directly aligns with our problem of selecting a subset of edges from the complete edge space. Each edge can be represented by a binary value: 1 signifying the edge is included in the final peptide and 0 indicating exclusion. For Binary PSO, 50 particles are instantiated with cognitive (c1) and social (c2) acceleration coefficients set to 2.5 and 0.5, respectively. Additionally, the inertia weight is set to 0.9.
The optimal number of particles in PSO can vary based on the complexity of the problem. We opted for a common starting point of 50 particles, striking a balance between exploration of the search space and exploitation of promising regions. The cognitive (c1) and social (c2) coefficients were determined empirically to further balance exploration and exploitation within the algorithm. We found that c1 = 2.5 and c2 = 0.5 achieved this balance effectively. The cognitive coefficient emphasizes individual exploration based on a particle's best prior position ($p_{best}$), while the social coefficient promotes convergence towards the best position found by any particle in its neighborhood ($g_{best}$). Finally, the inertia weight (w) was also tuned empirically to a value of 0.9. This parameter controls the momentum of particles, allowing them to maintain some exploration history while gradually decreasing over time to encourage convergence towards promising solutions in later stages of the optimization process.

\section{Training Methodology}
The model was trained using the Adam optimizer \cite{kingma2014adam}, with an initial learning rate of $10^{-3}$. To prevent overfitting and improve generalization, a data augmentation strategy was employed during training. This involved adding a small Gaussian noise with a standard deviation of 0.1 to the protein atom coordinates. The forward and reverse diffusion processes took place through 1000 steps. Additionally, a learning rate decay schedule was implemented to decay the learning rate exponentially with a factor of 0.6 towards a minimum value of $10^{-6}$ if there is no improvement in the validation loss for 10 consecutive steps. The model was trained using a batch size of 2, and to balance the contributions of different loss terms within the overall loss function, a factor of $\alpha = 100$ was multiplied onto the residue type loss. Figures concerning the training progression are provided below. (TODO: figures from supplementary) \\

To gain deeper insights into the training process, we tracked not only the total training and validation losses but also their individual components: position loss and feature loss. Position loss quantifies the error between the predicted continuous Cartesian coordinates $(x, y, z)$ and the ground truth. Feature loss, on the other hand, measures the discrepancy between the predicted categorical feature distribution and the actual distribution. Figures TODO and TODO illustrate the behavior of these loss components throughout training. \\

The deep diffusion model was trained on the Ada HPC Cluster with 4x NVIDIA GeForce RTX 2080 Ti GPUs using the Distributed Data Parallel (DDP) Strategy. All inference and reconstruction experiments were carried out on multiple nodes, each with 40x Intel Xeon E5-2640 v4 CPUs, 80 GB of RAM, and 1x NVIDIA GeForce RTX 2080 Ti GPU.

\section{Optimization of Evaluation Algorithm Parameters}
Autodock Vina was employed to compute the binding affinity between the reconstructed peptide and the target protein. Although docking was not performed, Vina's built-in scoring function served as the metric for evaluating both the reconstruction process and the final generated peptides. To facilitate Vina docking calculations, the reconstructed peptide's size (in Angstroms) and center coordinates were pre-computed and provided as input parameters. We opted to utilize the default settings within Vina for the exhaustiveness of the global search (set to 8) and the maximum number of binding modes to be generated (limited to 9). These defaults ensure a balance between computational efficiency and exploration of the search space. \\

For protein-peptide docking simulations, we utilized FRODOCK. However, prior to docking, the reconstructed peptides underwent preprocessing using the PDB2PQR tool. This preprocessing step ensured the peptides were properly prepared for docking by reconstructing missing atoms, adding hydrogens, and assigning atomic charges and radii based on the CHARMM force field. Following this preparation step, FRODOCK simulations were conducted using the Rosetta force field and the default SOAP potentials. Docking simulations can be computationally expensive, so to improve efficiency, the process was parallelized across 8 threads using OpenMP. This parallelization strategy significantly reduced the overall computational time required for the docking simulations.

\section{Application to PfEMP1 Protein Targets}
\textit{Plasmodium falciparum}, the causative agent of the most severe form of malaria, expresses a diverse family of PfEMP1 proteins. These highly polymorphic surface antigens, comprising approximately 60 known variants, play a critical role in severe malaria pathogenesis by mediating the cytoadherence of infected erythrocytes to endothelial receptors. This adherence leads to sequestration and contributes to tissue damage \cite{jensen2020cerebral}. PfEMP1 is a pivotal virulence factor secreted by the malaria parasite. It binds to the erythrocyte membrane, triggering the binding of red blood cells (RBCs) to blood vessels \cite{pasternak2009pfemp1}. By obstructing tiny blood arteries, they exacerbate malaria infections and increase the risk of cerebral malaria, placental malaria, and severe anemia \cite{jensen2020cerebral}. The parasite can avoid the host immune system because of this antigenic diversity of PfEMP1 family genes, since new infection can express different PfEMP1 variants that are not recognized by preexisting immune responses \cite{scherf2008antigenic}.
We leveraged single-cell RNA-seq data to identify the five most highly expressed PfEMP1 genes. Using CAVITY \cite{Yuan2013}, we predicted strong and medium druggable binding sites within the proteins encoded by these genes (Figure TODO). Subsequently, HYDRA was used to design potential peptide molecules specifically targeted to these binding sites (Figure TODO). We also checked the binding affinities of generated peptides of one with other proteins (heatmap TODO).  During analysis of the protein structures, we focused on extracellular and intracellular domains for peptide generation, leveraging cavities as potential binding pockets. Finally, we evaluated the binding affinities between the designed peptides and their target proteins on these PfEMP1 variants.
